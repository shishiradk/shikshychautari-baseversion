from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import Response, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import uuid
import json
import re
import asyncio
from io import BytesIO
import hashlib
from datetime import datetime

# Better PDF parsing with PyMuPDF
try:
    import fitz  # PyMuPDF
    PYMUPDF_OK = True
except ImportError:
    PYMUPDF_OK = False
    print("‚ö†Ô∏è PyMuPDF not available. Install with: pip install PyMuPDF")

# Fallback to PyPDF2
try:
    from PyPDF2 import PdfReader
    PYPDF2_OK = True
except ImportError:
    PYPDF2_OK = False

from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from dotenv import load_dotenv
import boto3
import tempfile

# Optional PDF generation (reportlab)
try:
    from reportlab.lib.pagesizes import A4
    from reportlab.lib.styles import getSampleStyleSheet
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    REPORTLAB_OK = True
except Exception:
    REPORTLAB_OK = False

# Load environment variables
load_dotenv()

# LangSmith Tracking
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "Question Paper Generator API"

# Pydantic Models
class GeneratePaperRequest(BaseModel):
    bucket_name: str
    old_question_prefix: str
    syllabus_prefix: str
    predicted_question_prefix: str
    additional_instructions: Optional[str] = None

class PaperOutline(BaseModel):
    title: str
    duration_minutes: int
    total_marks: int
    sections: List[Dict[str, Any]]

# FastAPI app initialization
app = FastAPI(
    title="üéì Enhanced Question Paper Generator API",
    description="""
    ## üìö AI-Powered Question Paper Prediction (Enhanced)
    
    Upload your **syllabus PDFs** and **old question papers**, and get a **predicted question paper** generated by AI.
    
    ### ‚ú® Enhanced Features:
    - ü§ñ **Two-Phase Generation**: Outline ‚Üí Draft per section
    - üìÑ **Better PDF Parsing**: PyMuPDF with layout preservation
    - üéØ **MMR Retrieval**: Diverse context selection
    - ‚úÖ **Validation & Repair**: Automatic error checking and fixing
    - üöÄ **Token-Aware Chunking**: Better text segmentation
    - üìä **Structured Output**: JSON outlines with validation
    
    ### üöÄ How to Use:
    1. Click on the **POST /generate-paper** endpoint below
    2. Upload your PDF files (syllabus + old questions)
    3. Add optional instructions if needed
    4. Click **Execute** and download your predicted paper!
    
    ---
    """,
    version="3.0.0",
    docs_url="/swagger",
    redoc_url="/redoc",
    contact={
        "name": "Question Paper Generator",
        "url": "https://github.com/your-repo",
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT",
    },
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001", 
        "http://localhost:3002",
        "http://localhost:3003",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:3001",
        "http://127.0.0.1:3002", 
        "http://127.0.0.1:3003"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# S3 Utility functions
def get_s3_client():
    """Get S3 client with credentials"""
    return boto3.client(
        's3',
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        region_name=os.getenv("AWS_REGION", "eu-north-1")
    )

def download_pdfs_from_s3(bucket_name: str, prefix: str) -> List[bytes]:
    """Download all PDF files from S3 bucket with given prefix"""
    s3_client = get_s3_client()
    pdf_contents = []
    temp_files = []  # Keep track of temp files for cleanup
    
    try:
        # List objects with the prefix
        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        
        if 'Contents' not in response:
            raise HTTPException(status_code=404, detail=f"No files found in bucket {bucket_name} with prefix {prefix}")
        
        # Download each PDF file
        for obj in response['Contents']:
            key = obj['Key']
            if key.lower().endswith('.pdf'):
                print(f"üì• Downloading: {key}")
                
                # Create temporary file
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
                temp_files.append(tmp_file.name)
                tmp_file.close()
                
                try:
                    # Download file to temporary location
                    s3_client.download_file(bucket_name, key, tmp_file.name)
                    
                    # Read the file content
                    with open(tmp_file.name, 'rb') as f:
                        pdf_content = f.read()
                        pdf_contents.append(pdf_content)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing {key}: {str(e)}")
                    continue
        
        if not pdf_contents:
            raise HTTPException(status_code=404, detail=f"No PDF files found in bucket {bucket_name} with prefix {prefix}")
        
        print(f"‚úÖ Downloaded {len(pdf_contents)} PDF files from S3")
        return pdf_contents
        
    except Exception as e:
        if "NoSuchBucket" in str(e):
            raise HTTPException(status_code=404, detail=f"Bucket {bucket_name} not found")
        elif "AccessDenied" in str(e):
            raise HTTPException(status_code=403, detail=f"Access denied to bucket {bucket_name}")
        else:
            raise HTTPException(status_code=500, detail=f"Error downloading from S3: {str(e)}")
    
    finally:
        # Clean up all temporary files
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                print(f"‚ö†Ô∏è Could not delete temp file {temp_file}: {str(e)}")
                # Continue with cleanup even if one file fails

def upload_pdf_to_s3(pdf_bytes: bytes, bucket_name: str, output_prefix: str, filename: str) -> str:
    """Upload PDF to S3 and return the full S3 key"""
    s3_client = get_s3_client()
    
    # Create the full S3 key
    s3_key = f"{output_prefix.rstrip('/')}/{filename}"
    
    try:
        s3_client.put_object(
            Bucket=bucket_name,
            Key=s3_key,
            Body=pdf_bytes,
            ContentType='application/pdf'
        )
        
        print(f"‚úÖ Uploaded PDF to S3: {s3_key}")
        return s3_key
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error uploading to S3: {str(e)}")

# Enhanced PDF text extraction
def extract_clean_text(pdf_bytes: bytes) -> str:
    """Extract clean text using PyMuPDF with layout preservation"""
    if PYMUPDF_OK:
        try:
            with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                blocks = []
                for page in doc:
                    for block in page.get_text("blocks"):
                        blocks.append(block[4])
                text = "\n".join(blocks)
                
                # Clean up the text
                text = re.sub(r"-\n", "", text)             # de-hyphenate
                text = re.sub(r"[ \t]+", " ", text)         # normalize whitespace
                text = re.sub(r"\n{3,}", "\n\n", text)      # reduce multiple newlines
                text = re.sub(r"^\s+|\s+$", "", text, flags=re.MULTILINE)  # trim lines
                return text.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è PyMuPDF failed: {e}, falling back to PyPDF2")
    
    # Fallback to PyPDF2
    if PYPDF2_OK:
        try:
            reader = PdfReader(BytesIO(pdf_bytes))
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            return text.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è PyPDF2 failed: {e}")
    
    raise RuntimeError("No PDF parser available. Install PyMuPDF: pip install PyMuPDF")

def extract_text_from_pdfs(pdfs: List[bytes]) -> str:
    """Extract text from multiple PDF files with enhanced parsing"""
    full_text = ""
    for pdf_bytes in pdfs:
        text = extract_clean_text(pdf_bytes)
        if text:
            full_text += text + "\n\n"
    return full_text.strip()

# Enhanced text chunking with token awareness
def chunk_text(text: str) -> List[str]:
    """Split text into token-aware chunks for better retrieval"""
    try:
        # Use token-based splitter for better chunking
        splitter = TokenTextSplitter(
            chunk_size=800, 
            chunk_overlap=120, 
            encoding_name="cl100k_base"
        )
        return splitter.split_text(text)
    except Exception:
        # Fallback to character-based splitter
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        return splitter.split_text(text)

# Enhanced vector store with MMR retrieval
def create_vector_store(chunks: List[str], session_id: str) -> str:
    """Create and save vector store for a session with MMR support"""
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_texts(chunks, embedding=embeddings)
    
    # Create session-specific directory
    db_path = f"faiss_index_{session_id}"
    vectorstore.save_local(db_path)
    return db_path

def load_vector_store(session_id: str) -> FAISS:
    """Load vector store for a session"""
    embeddings = OpenAIEmbeddings()
    db_path = f"faiss_index_{session_id}"
    return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)

# Two-phase generation: Outline + Draft
def get_outline_generator_chain():
    """Create the outline generation chain"""
    template = """
You are a strict exam setter. Analyze the past question patterns and syllabus to create a structured outline.

Produce ONLY this JSON schema:
{{
  "title": "string",
  "duration_minutes": integer,
  "total_marks": integer,
  "sections": [
    {{
      "name": "string",
      "questions": [
        {{
          "id": "string",
          "marks": integer,
          "type": "short|long|mcq",
          "topic": "string"
        }}
      ]
    }}
  ]
}}

Constraints:
1. Mirror the sections, numbering, and marks distribution seen in past papers
2. Match the total marks exactly
3. Include high-importance syllabus areas that were underrepresented historically
4. Maintain realistic exam structure and timing

--- PAST QUESTION CONTEXT ---
{past_questions}

--- SYLLABUS CONTENT ---
{syllabus}

{additional_instructions}

Output ONLY the valid JSON, no other text.
"""
    prompt = PromptTemplate(
        input_variables=["past_questions", "syllabus", "additional_instructions"],
        template=template,
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2, max_tokens=800)
    output_parser = JsonOutputParser()
    return prompt | llm | output_parser

def get_section_generator_chain():
    """Create the section generation chain"""
    template = """
Fill Section "{section_name}" with {question_count} questions totaling {total_marks} marks.

Use the retrieved context and syllabus to generate questions. Output only the finalized section text.

Rules:
1. Stick to the exact marks and question counts from the outline
2. No verbatim reuse from past papers
3. Maintain academic tone and clarity
4. Ensure questions align with the specified topics
5. Follow the exam structure and format

--- RETRIEVED CONTEXT ---
{context}

--- SYLLABUS CONTENT ---
{syllabus}

--- SECTION OUTLINE ---
{section_outline}

Output only the section text, no explanations.
"""
    prompt = PromptTemplate(
        input_variables=["section_name", "question_count", "total_marks", "context", "syllabus", "section_outline"],
        template=template,
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.4, max_tokens=1000)
    output_parser = StrOutputParser()
    return prompt | llm | output_parser

# Enhanced retrieval with MMR
def get_retrievers(past_db: FAISS):
    """Create specialized retrievers for structure and content"""
    # Structure retriever for exam format
    structure_retriever = past_db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 8,
            "fetch_k": 20,
            "lambda_mult": 0.6
        }
    )
    
    # Content retriever for topic-specific questions
    content_retriever = past_db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 12,
            "fetch_k": 30,
            "lambda_mult": 0.4
        }
    )
    
    return structure_retriever, content_retriever

# Validator and repair system
def validate_paper_outline(outline: Dict[str, Any], syllabus_text: str) -> Dict[str, Any]:
    """Validate the generated paper outline"""
    errors = []
    warnings = []
    
    # Check marks consistency
    total_calculated = sum(
        sum(q["marks"] for q in section["questions"])
        for section in outline["sections"]
    )
    
    if total_calculated != outline["total_marks"]:
        errors.append(f"marks_total_mismatch: calculated {total_calculated}, declared {outline['total_marks']}")
    
    # Check section structure
    if not outline["sections"]:
        errors.append("no_sections_defined")
    
    for i, section in enumerate(outline["sections"]):
        if not section.get("questions"):
            errors.append(f"section_{i}_no_questions")
        
        section_marks = sum(q["marks"] for q in section["questions"])
        if section_marks == 0:
            errors.append(f"section_{i}_zero_marks")
    
    # Check duration and marks ratio (typical: 1 mark per minute)
    expected_duration = outline["total_marks"] * 1.2  # Allow some buffer
    if outline["duration_minutes"] < expected_duration * 0.8:
        warnings.append(f"duration_may_be_too_short: {outline['duration_minutes']} min for {outline['total_marks']} marks")
    
    return {
        "valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings,
        "total_calculated": total_calculated,
        "coverage_score": 0.85  # Placeholder for syllabus coverage
    }

def repair_outline(outline: Dict[str, Any], validation_result: Dict[str, Any]) -> Dict[str, Any]:
    """Repair the outline based on validation errors"""
    if validation_result["valid"]:
        return outline
    
    # Simple repair: fix marks mismatch
    if "marks_total_mismatch" in str(validation_result["errors"]):
        total_calculated = validation_result["total_calculated"]
        outline["total_marks"] = total_calculated
        print(f"üîß Fixed marks mismatch: updated total to {total_calculated}")
    
    return outline

# Two-phase generation
async def generate_paper_outline(past_db: FAISS, syllabus_text: str, additional_instructions: str = "") -> Dict[str, Any]:
    """Phase 1: Generate paper outline"""
    print("üìã Phase 1: Generating paper outline...")
    
    # Get structure context
    structure_retriever, _ = get_retrievers(past_db)
    structure_docs = structure_retriever.get_relevant_documents(
        "exam structure sections numbering marks distribution typical topics"
    )
    structure_context = "\n".join([doc.page_content for doc in structure_docs])
    
    # Generate outline
    chain = get_outline_generator_chain()
    try:
        outline = chain.invoke({
            "past_questions": structure_context,
            "syllabus": syllabus_text,
            "additional_instructions": additional_instructions
        })
        
        # Validate and repair
        validation = validate_paper_outline(outline, syllabus_text)
        if not validation["valid"]:
            print(f"‚ö†Ô∏è Outline validation failed: {validation['errors']}")
            outline = repair_outline(outline, validation)
            validation = validate_paper_outline(outline, syllabus_text)
        
        print(f"‚úÖ Outline generated: {outline['total_marks']} marks, {len(outline['sections'])} sections")
        return outline
        
    except Exception as e:
        print(f"‚ùå Outline generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Outline generation failed: {str(e)}")

async def generate_paper_sections(
    outline: Dict[str, Any], 
    past_db: FAISS, 
    syllabus_text: str
) -> str:
    """Phase 2: Generate each section"""
    print("üìù Phase 2: Generating paper sections...")
    
    _, content_retriever = get_retrievers(past_db)
    chain = get_section_generator_chain()
    
    sections = []
    
    for section in outline["sections"]:
        section_name = section["name"]
        question_count = len(section["questions"])
        total_marks = sum(q["marks"] for q in section["questions"])
        
        print(f"  üìÑ Generating section: {section_name} ({question_count} questions, {total_marks} marks)")
        
        # Get relevant context for this section
        context_docs = content_retriever.get_relevant_documents(
            f"{section_name} questions topics content"
        )
        context = "\n".join([doc.page_content for doc in context_docs])
        
        # Generate section
        section_text = chain.invoke({
            "section_name": section_name,
            "question_count": question_count,
            "total_marks": total_marks,
            "context": context,
            "syllabus": syllabus_text,
            "section_outline": json.dumps(section, indent=2)
        })
        
        sections.append(f"## {section_name}\n{section_text}\n")
    
    # Combine sections
    full_paper = f"# {outline['title']}\n\n"
    full_paper += f"**Duration:** {outline['duration_minutes']} minutes\n"
    full_paper += f"**Total Marks:** {outline['total_marks']}\n\n"
    full_paper += "\n".join(sections)
    
    print("‚úÖ All sections generated successfully")
    return full_paper

# Main generation function
async def generate_predicted_paper_enhanced(past_db: FAISS, syllabus_text: str, additional_instructions: str = "") -> str:
    """Enhanced two-phase question paper generation"""
    try:
        # Phase 1: Generate outline
        outline = await generate_paper_outline(past_db, syllabus_text, additional_instructions)
        
        # Phase 2: Generate sections
        paper = await generate_paper_sections(outline, past_db, syllabus_text)
        
        return paper
        
    except Exception as e:
        print(f"‚ùå Enhanced generation failed: {e}")
        # Fallback to original method
        print("üîÑ Falling back to original generation method...")
        return generate_predicted_paper_original(past_db, syllabus_text, additional_instructions)

# Original generation method as fallback
def generate_predicted_paper_original(past_db: FAISS, syllabus_text: str, additional_instructions: str = "") -> str:
    """Original question generation method as fallback"""
    template = """
You are an expert academic exam paper predictor.
Generate exactly **one** complete future exam paper with the highest probability of appearing questions, using the inputs below.

Requirements:
1) Mirror the sections, numbering, and marks distribution seen in past papers.
2) Maximize recurrence likelihood: pick topics and phrasings consistent with past patterns without copying verbatim.
3) Fill all sections fully. Produce one paper only.
4) Include high-importance syllabus areas that were underrepresented historically to keep the paper realistic.
5) Maintain academic tone, clarity, and logical flow.

--- PAST QUESTION CONTEXT ---
{past_questions}

--- SYLLABUS CONTENT ---
{syllabus}

{additional_instructions}

Output the predicted exam paper only.
"""
    prompt = PromptTemplate(
        input_variables=["past_questions", "syllabus", "additional_instructions"],
        template=template,
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, max_tokens=1400)
    output_parser = StrOutputParser()
    chain = prompt | llm | output_parser
    
    # Retrieve rich past context to drive structure + topic frequencies
    docs = past_db.similarity_search("exam structure sections numbering marks distribution typical topics", k=12)
    past_context = "\n".join([doc.page_content for doc in docs])
    
    additional_prompt = f"\nAdditional Instructions: {additional_instructions}" if additional_instructions else ""
    
    return chain.invoke({
        "past_questions": past_context,
        "syllabus": syllabus_text,
        "additional_instructions": additional_prompt
    })

def paper_to_pdf_bytes(title: str, content: str) -> bytes:
    """Convert paper content to PDF bytes"""
    if not REPORTLAB_OK:
        raise RuntimeError("ReportLab not installed. Run: pip install reportlab")
    
    buf = BytesIO()
    doc = SimpleDocTemplate(buf, pagesize=A4, title=title)
    styles = getSampleStyleSheet()
    story = [Paragraph(title, styles["Title"]), Spacer(1, 12)]
    
    # Simple text processing (can be enhanced)
    lines = content.split('\n')
    for line in lines:
        if line.strip():
            story.append(Paragraph(line.strip(), styles["BodyText"]))
            story.append(Spacer(1, 6))
    
    doc.build(story)
    pdf_data = buf.getvalue()
    buf.close()
    return pdf_data

# API Endpoints
@app.get("/", tags=["Info"])
async def root():
    """
    ## üè† API Information
    
    Welcome to the Enhanced Question Paper Generator API! This API uses AI to predict future exam questions.
    """
    return {
        "üéì": "Enhanced Question Paper Generator API",
        "version": "3.0.0",
        "üìñ": "Interactive docs at /swagger",
        "üöÄ": "Main endpoint: POST /generate-paper",
        "üí°": "Upload syllabus + old questions ‚Üí Get predicted paper PDF",
        "ü§ñ": "Powered by OpenAI GPT-4 + LangChain",
        "‚ú®": "Enhanced with two-phase generation, validation, and MMR retrieval"
    }

@app.post("/generate-paper", 
           tags=["üéØ Question Paper Generation"],
           summary="üöÄ Generate Predicted Question Paper (File Upload)",
           description="Upload syllabus and old question PDFs to generate a predicted exam paper",
           response_description="PDF file containing the predicted question paper")
async def generate_paper_from_files(
    syllabus_files: List[UploadFile] = File(
        ..., 
        description="üìö **Syllabus PDF Files** - Upload one or more PDF files containing syllabus/course content"
    ),
    old_question_files: List[UploadFile] = File(
        ..., 
        description="üìÑ **Old Question Papers** - Upload one or more PDF files with past exam questions"
    ),
    additional_instructions: Optional[str] = Form(
        None,
        description="üí¨ **Additional Instructions** (Optional) - Specific requirements like 'Focus on practical questions' or 'Include numerical problems'"
    )
):
    """
    ## üéì Generate AI-Predicted Question Paper (Enhanced)
    
    This endpoint analyzes your syllabus and past question papers to generate a predicted future exam paper.
    
    ### üìã What you need:
    - **Syllabus PDFs**: Course syllabus, curriculum, or study materials
    - **Old Question Papers**: Previous years' exam papers or practice questions
    
    ### ü§ñ What it does:
    1. **Extracts text** from all uploaded PDFs (enhanced with PyMuPDF)
    2. **Analyzes patterns** in past questions using AI with MMR retrieval
    3. **Generates outline** with validation and repair
    4. **Creates sections** with targeted context retrieval
    5. **Returns** a formatted PDF ready for download
    
    ### ‚ö° Enhanced Features:
    - Two-phase generation (Outline ‚Üí Draft)
    - MMR retrieval for diverse context
    - Automatic validation and repair
    - Token-aware text chunking
    - Better PDF parsing with PyMuPDF
    
    ---
    **Returns:** PDF file containing your predicted question paper
    """
    try:
        # Generate unique session ID for this request
        session_id = str(uuid.uuid4())
        
        # Validate and process syllabus files
        syllabus_pdf_data = []
        for file in syllabus_files:
            if not file.filename.lower().endswith('.pdf'):
                raise HTTPException(status_code=400, detail=f"Syllabus file {file.filename} is not a PDF")
            content = await file.read()
            syllabus_pdf_data.append(content)
        
        # Validate and process old question files
        old_question_pdf_data = []
        for file in old_question_files:
            if not file.filename.lower().endswith('.pdf'):
                raise HTTPException(status_code=400, detail=f"Old question file {file.filename} is not a PDF")
            content = await file.read()
            old_question_pdf_data.append(content)
        
        # Extract text from syllabus PDFs
        syllabus_text = extract_text_from_pdfs(syllabus_pdf_data)
        if not syllabus_text.strip():
            raise HTTPException(status_code=400, detail="No text could be extracted from syllabus PDF files")
        
        # Extract text from old question PDFs
        old_questions_text = extract_text_from_pdfs(old_question_pdf_data)
        if not old_questions_text.strip():
            raise HTTPException(status_code=400, detail="No text could be extracted from old question PDF files")
        
        # Create chunks and vector store
        chunks = chunk_text(old_questions_text)
        vector_store_path = create_vector_store(chunks, session_id)
        
        # Load vector store and generate paper
        past_db = load_vector_store(session_id)
        predicted_paper = await generate_predicted_paper_enhanced(
            past_db, 
            syllabus_text, 
            additional_instructions or ""
        )
        
        # Generate PDF
        if not REPORTLAB_OK:
            raise HTTPException(
                status_code=501, 
                detail="PDF generation not available. Install reportlab: pip install reportlab"
            )
        
        pdf_bytes = paper_to_pdf_bytes("Predicted Question Paper", predicted_paper)
        
        # Clean up vector store files
        try:
            import shutil
            if os.path.exists(vector_store_path):
                shutil.rmtree(vector_store_path)
        except Exception:
            pass  # Ignore cleanup errors
        
        # Return PDF as response
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=predicted_paper_{session_id[:8]}.pdf",
                "Content-Length": str(len(pdf_bytes))
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        # Clean up on error
        try:
            import shutil
            vector_store_path = f"faiss_index_{session_id}"
            if os.path.exists(vector_store_path):
                shutil.rmtree(vector_store_path)
        except Exception:
            pass
        
        raise HTTPException(status_code=500, detail=f"Error generating question paper: {str(e)}")

@app.post("/generate-paper-s3", 
           tags=["‚òÅÔ∏è S3 Question Paper Generation"],
           summary="‚òÅÔ∏è Generate from S3 and Upload to S3",
           description="Download PDFs from S3 (old questions + syllabus), generate question paper, and upload result back to S3",
           response_description="Success message with S3 location of generated PDF")
async def generate_paper_from_s3(
    data: GeneratePaperRequest
):
    """
    ## ‚òÅÔ∏è Generate Question Paper from S3 and Upload Back (Enhanced)
    
    This endpoint downloads old question PDFs AND syllabus PDFs from S3, generates a predicted question paper, 
    and uploads the result back to S3 using the provided predicted question prefix.
    
    ### üìã What you need:
    - **S3 Bucket**: Contains old question PDFs and syllabus PDFs
    - **Old Question Prefix**: Where old questions are stored in S3
    - **Syllabus Prefix**: Where syllabus PDFs are stored in S3
    - **Predicted Question Prefix**: Where generated PDF will be uploaded
    
    ### ü§ñ Enhanced Process:
    1. **Downloads** PDFs from S3 with better parsing (PyMuPDF)
    2. **Generates outline** with validation and repair
    3. **Creates sections** with MMR retrieval
    4. **Uploads** result to S3
    5. **Returns** success message with S3 location
    
    ---
    **Returns:** Success message with S3 location of generated PDF
    """
    try:
        # Generate unique session ID for this request
        session_id = str(uuid.uuid4())
        
        print(f"üöÄ Starting Enhanced S3-based question paper generation...")
        print(f"üì¶ Bucket: {data.bucket_name}")
        print(f"üì• Old Question prefix: {data.old_question_prefix}")
        print(f"üìö Syllabus prefix: {data.syllabus_prefix}")
        print(f"üì§ Predicted Question prefix: {data.predicted_question_prefix}")
        
        # Step 1: Download old question PDFs from S3
        print("üì• Downloading old question PDFs from S3...")
        old_question_pdfs = download_pdfs_from_s3(data.bucket_name, data.old_question_prefix)
        
        # Step 2: Download syllabus PDFs from S3
        print("üìö Downloading syllabus PDFs from S3...")
        syllabus_pdfs = download_pdfs_from_s3(data.bucket_name, data.syllabus_prefix)
        
        # Step 3: Extract text from all PDFs
        print("üîç Extracting text from PDFs...")
        syllabus_text = extract_text_from_pdfs(syllabus_pdfs)
        if not syllabus_text.strip():
            raise HTTPException(status_code=400, detail="No text could be extracted from syllabus PDF files")
        
        old_questions_text = extract_text_from_pdfs(old_question_pdfs)
        if not old_questions_text.strip():
            raise HTTPException(status_code=400, detail="No text could be extracted from old question PDF files")
        
        # Step 4: Generate question paper
        print("ü§ñ Generating predicted question paper...")
        chunks = chunk_text(old_questions_text)
        vector_store_path = create_vector_store(chunks, session_id)
        
        past_db = load_vector_store(session_id)
        predicted_paper = await generate_predicted_paper_enhanced(
            past_db, 
            syllabus_text, 
            data.additional_instructions or ""
        )
        
        # Step 5: Generate PDF
        if not REPORTLAB_OK:
            raise HTTPException(
                status_code=501, 
                detail="PDF generation not available. Install reportlab: pip install reportlab"
            )
        
        pdf_bytes = paper_to_pdf_bytes("Predicted Question Paper", predicted_paper)
        
        # Step 6: Upload to S3 using the provided predicted question prefix
        print("üì§ Uploading generated PDF to S3...")
        timestamp = session_id[:8]
        filename = f"predicted_paper_{timestamp}.pdf"
        s3_key = upload_pdf_to_s3(pdf_bytes, data.bucket_name, data.predicted_question_prefix, filename)
        
        # Step 7: Clean up
        try:
            import shutil
            if os.path.exists(vector_store_path):
                shutil.rmtree(vector_store_path)
        except Exception:
            pass
        
        # Step 8: Return success response
        return JSONResponse(
            content={
                "success": True,
                "message": "Enhanced question paper generated and uploaded successfully!",
                "s3_bucket": data.bucket_name,
                "s3_key": s3_key,
                "s3_url": f"https://{data.bucket_name}.s3.amazonaws.com/{s3_key}",
                "file_size": len(pdf_bytes),
                "session_id": session_id,
                "predicted_question_prefix": data.predicted_question_prefix,
                "enhancements": [
                    "Two-phase generation (Outline ‚Üí Draft)",
                    "MMR retrieval for diverse context",
                    "Automatic validation and repair",
                    "Token-aware text chunking",
                    "Better PDF parsing with PyMuPDF"
                ]
            },
            status_code=200
        )
        
    except HTTPException:
        raise
    except Exception as e:
        # Clean up on error
        try:
            import shutil
            vector_store_path = f"faiss_index_{session_id}"
            if os.path.exists(vector_store_path):
                shutil.rmtree(vector_store_path)
        except Exception:
            pass
        
        raise HTTPException(status_code=500, detail=f"Error in S3-based generation: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    # Check if required environment variables are set
    missing_vars = []
    
    if not os.getenv("OPENAI_API_KEY"):
        missing_vars.append("OPENAI_API_KEY")
    
    if not os.getenv("AWS_ACCESS_KEY_ID"):
        missing_vars.append("AWS_ACCESS_KEY_ID")
    
    if not os.getenv("AWS_SECRET_ACCESS_KEY"):
        missing_vars.append("AWS_SECRET_ACCESS_KEY")
    
    if missing_vars:
        print("‚ö†Ô∏è  Warning: Missing required environment variables:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n   Please set these in a .env file or environment variables")
        print("   Example .env file:")
        print("   OPENAI_API_KEY=your_openai_key")
        print("   AWS_ACCESS_KEY_ID=your_aws_key")
        print("   AWS_SECRET_ACCESS_KEY=your_aws_secret")
        print("   AWS_REGION=eu-north-1")
        exit(1)
    
    print("üöÄ Starting Enhanced Question Paper Generator API...")
    print("üìö Swagger UI: http://localhost:8000/swagger")
    print("üîó API Root: http://localhost:8000/")
    print("‚ú® Enhanced with: Two-phase generation, validation, MMR retrieval")
    print("üìñ PDF Parser: PyMuPDF" if PYMUPDF_OK else "üìñ PDF Parser: PyPDF2 (fallback)")
    print("=" * 50)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
